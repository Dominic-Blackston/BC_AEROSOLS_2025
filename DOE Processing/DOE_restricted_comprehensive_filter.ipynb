{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2937fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.odr as odr\n",
    "import os\n",
    "import sys\n",
    "import arrow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from numpy import nan_to_num\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import os\n",
    "import sys\n",
    "import arrow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import seaborn as sns\n",
    "from datetime import date\n",
    "from numpy import nan_to_num\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1aa466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path definitions\n",
    "RAW_PATH = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\wavelength_corrected_data\"\n",
    "RESTRICTED_OUTPUT_PATH = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\restricted_campaigns\"\n",
    "COMPREHENSIVE_OUTPUT_PATH = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\comprehensive_campaigns\"\n",
    "\n",
    "\n",
    "RESTRICTED_COLUMNS = [\"Organization\", \"Campaign\", \"UTC\", \"Date\", \"Latitude\", \"Longitude\", \"Altitude\", \"Temperature\", \"Rel_humidity\", \"Pressure\", \"BC_Mass\", \"bin1\", \"bin2\", \"bin3\", \"bin4\", \"bin5\", \"bin6\", \"bin7\", \"bin8\", \"bin9\", \"bin10\", \"bin11\", \"bin12\", \"bin13\", \"bin14\", \"bin15\", \"Sc450_total\", \"Sc550_total\", \"Sc700_total\", \"Abs470_total\", \"Abs532_total\", \"Abs660_total\"]\n",
    "COMPREHENSIVE_COLUMNS = [\"Organization\", \"Campaign\", \"UTC\", \"Date\", \"Latitude\", \"Longitude\", \"Altitude\", \"Temperature\", \"Rel_humidity\", \"Pressure\", \"U\", \"V\", \"W\", \"Supersaturation\", \"Number_Concentration\", \"CNgt3nm\", \"CNgt10nm\", \"BC_Mass\", \"LWC\", \"cbin1\", \"cbin2\", \"cbin3\", \"cbin4\", \"cbin5\", \"cbin6\", \"cbin7\", \"cbin8\", \"cbin9\", \"cbin10\", \"bin1\", \"bin2\", \"bin3\", \"bin4\", \"bin5\", \"bin6\", \"bin7\", \"bin8\", \"bin9\", \"bin10\", \"bin11\", \"bin12\", \"bin13\", \"bin14\", \"bin15\", \"Sc450_total\", \"Sc550_total\", \"Sc700_total\", \"Abs470_total\", \"Abs532_total\", \"Abs660_total\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d67a26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COLUMN REQUIREMENTS SUMMARY:\n",
      "============================================================\n",
      "\n",
      "RESTRICTED columns (32 total):\n",
      "Organization, Campaign, UTC, Date, Latitude, Longitude, Altitude, Temperature, Rel_humidity, Pressure, BC_Mass, bin1, bin2, bin3, bin4, bin5, bin6, bin7, bin8, bin9, bin10, bin11, bin12, bin13, bin14, bin15, Sc450_total, Sc550_total, Sc700_total, Abs470_total, Abs532_total, Abs660_total\n",
      "\n",
      "COMPREHENSIVE columns (50 total):\n",
      "Organization, Campaign, UTC, Date, Latitude, Longitude, Altitude, Temperature, Rel_humidity, Pressure, U, V, W, Supersaturation, Number_Concentration, CNgt3nm, CNgt10nm, BC_Mass, LWC, cbin1, cbin2, cbin3, cbin4, cbin5, cbin6, cbin7, cbin8, cbin9, cbin10, bin1, bin2, bin3, bin4, bin5, bin6, bin7, bin8, bin9, bin10, bin11, bin12, bin13, bin14, bin15, Sc450_total, Sc550_total, Sc700_total, Abs470_total, Abs532_total, Abs660_total\n",
      "\n",
      "Additional columns in COMPREHENSIVE (not in RESTRICTED):\n",
      "U, V, W, Supersaturation, Number_Concentration, CNgt3nm, CNgt10nm, LWC, cbin1, cbin2, cbin3, cbin4, cbin5, cbin6, cbin7, cbin8, cbin9, cbin10\n",
      "\n",
      "Files to process (9 total):\n",
      "  ACE-ENA_wavelengthcorrected.csv → ACE-ENA_restricted.csv, ACE-ENA_comprehensive.csv\n",
      "  ACMEV_wavelengthcorrected.csv → ACMEV_restricted.csv, ACMEV_comprehensive.csv\n",
      "  BBOP_wavelengthcorrected.csv → BBOP_restricted.csv, BBOP_comprehensive.csv\n",
      "  CACTI_wavelengthcorrected.csv → CACTI_restricted.csv, CACTI_comprehensive.csv\n",
      "  CARES_wavelengthcorrected.csv → CARES_restricted.csv, CARES_comprehensive.csv\n",
      "  GOAMAZON_wavelengthcorrected.csv → GOAMAZON_restricted.csv, GOAMAZON_comprehensive.csv\n",
      "  ISDAC_wavelengthcorrected.csv → ISDAC_restricted.csv, ISDAC_comprehensive.csv\n",
      "  TCAP2012_wavelengthcorrected.csv → TCAP2012_restricted.csv, TCAP2012_comprehensive.csv\n",
      "  TCAP2013_wavelengthcorrected.csv → TCAP2013_restricted.csv, TCAP2013_comprehensive.csv\n",
      "============================================================\n",
      "PROCESSING CAMPAIGN FILES:\n",
      "============================================================\n",
      "Found 9 files to process\n",
      "\n",
      "Processing: ACE-ENA_wavelengthcorrected.csv\n",
      "  Campaign name: ACE-ENA\n",
      "  Loaded data with 546787 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 546787 rows with missing values (0/546787 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: ACE-ENA_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: ACE-ENA_comprehensive.csv\n",
      "\n",
      "Processing: ACMEV_wavelengthcorrected.csv\n",
      "  Campaign name: ACMEV\n",
      "  Loaded data with 55849 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 48881 rows with missing values (6968/55849 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: ACMEV_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: ACMEV_comprehensive.csv\n",
      "\n",
      "Processing: BBOP_wavelengthcorrected.csv\n",
      "  Campaign name: BBOP\n",
      "  Loaded data with 64847 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 57161 rows with missing values (7686/64847 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: BBOP_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: BBOP_comprehensive.csv\n",
      "\n",
      "Processing: CACTI_wavelengthcorrected.csv\n",
      "  Campaign name: CACTI\n",
      "  Loaded data with 38596 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 37354 rows with missing values (1242/38596 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: CACTI_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: CACTI_comprehensive.csv\n",
      "\n",
      "Processing: CARES_wavelengthcorrected.csv\n",
      "  Campaign name: CARES\n",
      "  Loaded data with 2838 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 2616 rows with missing values (222/2838 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: CARES_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: CARES_comprehensive.csv\n",
      "\n",
      "Processing: GOAMAZON_wavelengthcorrected.csv\n",
      "  Campaign name: GOAMAZON\n",
      "  Loaded data with 331893 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 331893 rows with missing values (0/331893 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: GOAMAZON_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: GOAMAZON_comprehensive.csv\n",
      "\n",
      "Processing: ISDAC_wavelengthcorrected.csv\n",
      "  Campaign name: ISDAC\n",
      "  Loaded data with 441569 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 441569 rows with missing values (0/441569 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: ISDAC_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: ISDAC_comprehensive.csv\n",
      "\n",
      "Processing: TCAP2012_wavelengthcorrected.csv\n",
      "  Campaign name: TCAP2012\n",
      "  Loaded data with 15523 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 15523 rows with missing values (0/15523 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: TCAP2012_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: TCAP2012_comprehensive.csv\n",
      "\n",
      "Processing: TCAP2013_wavelengthcorrected.csv\n",
      "  Campaign name: TCAP2013\n",
      "  Loaded data with 18901 rows and 50 columns\n",
      "  Creating restricted version...\n",
      "    Removed 10257 rows with missing values (8644/18901 rows remaining)\n",
      "    Existing columns: 32/32\n",
      "    Saved: TCAP2013_restricted.csv\n",
      "  Creating comprehensive version...\n",
      "    Existing columns: 50/50\n",
      "    Saved: TCAP2013_comprehensive.csv\n",
      "\n",
      "============================================================\n",
      "CAMPAIGN FILTERING COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def filter_dataframe_columns(df, required_columns):\n",
    "    \"\"\"\n",
    "    Filter dataframe to only include required columns in the specified order.\n",
    "    Creates missing columns with NaN values if they don't exist.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataframe\n",
    "    required_columns (list): List of required column names in desired order\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: Filtered dataframe with only required columns\n",
    "    \"\"\"\n",
    "    filtered_df = pd.DataFrame()\n",
    "    \n",
    "    for col in required_columns:\n",
    "        if col in df.columns:\n",
    "            # Column exists, copy it\n",
    "            filtered_df[col] = df[col]\n",
    "        else:\n",
    "            # Column doesn't exist, create it with NaN values\n",
    "            filtered_df[col] = pd.NA\n",
    "            \n",
    "    return filtered_df\n",
    "\n",
    "def extract_campaign_name(filename):\n",
    "    \"\"\"\n",
    "    Extract campaign name from filename by removing common suffixes.\n",
    "    \n",
    "    Parameters:\n",
    "    filename (str): Input filename\n",
    "    \n",
    "    Returns:\n",
    "    str: Campaign name\n",
    "    \"\"\"\n",
    "    # Remove file extension\n",
    "    base_name = os.path.splitext(filename)[0]\n",
    "    \n",
    "    # Remove common suffixes\n",
    "    suffixes_to_remove = [\n",
    "        '_wavelengthcorrected',\n",
    "        '_binned',\n",
    "        '_unit_corrected',\n",
    "        '_processed',\n",
    "        '_final'\n",
    "    ]\n",
    "    \n",
    "    campaign_name = base_name\n",
    "    for suffix in suffixes_to_remove:\n",
    "        if campaign_name.endswith(suffix):\n",
    "            campaign_name = campaign_name[:-len(suffix)]\n",
    "            break\n",
    "    \n",
    "    return campaign_name\n",
    "\n",
    "def process_campaign_files():\n",
    "    \"\"\"\n",
    "    Process all campaign files in RAW_PATH and create restricted and comprehensive versions.\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"PROCESSING CAMPAIGN FILES:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(RESTRICTED_OUTPUT_PATH, exist_ok=True)\n",
    "    os.makedirs(COMPREHENSIVE_OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Get all CSV files in RAW_PATH\n",
    "    campaign_files = glob.glob(os.path.join(RAW_PATH, \"*.csv\"))\n",
    "    \n",
    "    if not campaign_files:\n",
    "        print(\"No CSV files found in RAW_PATH\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(campaign_files)} files to process\")\n",
    "    \n",
    "    for file_path in campaign_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        campaign_name = extract_campaign_name(filename)\n",
    "        \n",
    "        print(f\"\\nProcessing: {filename}\")\n",
    "        print(f\"  Campaign name: {campaign_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the campaign data\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  Loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            \n",
    "            # Create restricted version\n",
    "            print(f\"  Creating restricted version...\")\n",
    "            restricted_df = filter_dataframe_columns(df, RESTRICTED_COLUMNS)\n",
    "            \n",
    "            # Remove rows with any empty/NaN values for restricted dataset\n",
    "            rows_before = len(restricted_df)\n",
    "            restricted_df = restricted_df.dropna()\n",
    "            rows_after = len(restricted_df)\n",
    "            rows_removed = rows_before - rows_after\n",
    "            \n",
    "            print(f\"    Removed {rows_removed} rows with missing values ({rows_after}/{rows_before} rows remaining)\")\n",
    "            \n",
    "            # Count existing vs missing columns for restricted\n",
    "            existing_restricted = sum(1 for col in RESTRICTED_COLUMNS if col in df.columns)\n",
    "            missing_restricted = len(RESTRICTED_COLUMNS) - existing_restricted\n",
    "            \n",
    "            print(f\"    Existing columns: {existing_restricted}/{len(RESTRICTED_COLUMNS)}\")\n",
    "            if missing_restricted > 0:\n",
    "                missing_cols = [col for col in RESTRICTED_COLUMNS if col not in df.columns]\n",
    "                print(f\"    Missing columns (filled with NaN): {missing_cols}\")\n",
    "            \n",
    "            # Save restricted version\n",
    "            restricted_filename = f\"{campaign_name}_restricted.csv\"\n",
    "            restricted_path = os.path.join(RESTRICTED_OUTPUT_PATH, restricted_filename)\n",
    "            restricted_df.to_csv(restricted_path, index=False)\n",
    "            print(f\"    Saved: {restricted_filename}\")\n",
    "            \n",
    "            # Create comprehensive version\n",
    "            print(f\"  Creating comprehensive version...\")\n",
    "            comprehensive_df = filter_dataframe_columns(df, COMPREHENSIVE_COLUMNS)\n",
    "            \n",
    "            # Count existing vs missing columns for comprehensive\n",
    "            existing_comprehensive = sum(1 for col in COMPREHENSIVE_COLUMNS if col in df.columns)\n",
    "            missing_comprehensive = len(COMPREHENSIVE_COLUMNS) - existing_comprehensive\n",
    "            \n",
    "            print(f\"    Existing columns: {existing_comprehensive}/{len(COMPREHENSIVE_COLUMNS)}\")\n",
    "            if missing_comprehensive > 0:\n",
    "                missing_cols = [col for col in COMPREHENSIVE_COLUMNS if col not in df.columns]\n",
    "                print(f\"    Missing columns (filled with NaN): {missing_cols}\")\n",
    "            \n",
    "            # Save comprehensive version\n",
    "            comprehensive_filename = f\"{campaign_name}_comprehensive.csv\"\n",
    "            comprehensive_path = os.path.join(COMPREHENSIVE_OUTPUT_PATH, comprehensive_filename)\n",
    "            comprehensive_df.to_csv(comprehensive_path, index=False)\n",
    "            print(f\"    Saved: {comprehensive_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {filename}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "def display_summary():\n",
    "    \"\"\"\n",
    "    Display summary of available files and column requirements.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COLUMN REQUIREMENTS SUMMARY:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nRESTRICTED columns ({len(RESTRICTED_COLUMNS)} total):\")\n",
    "    print(\", \".join(RESTRICTED_COLUMNS))\n",
    "    \n",
    "    print(f\"\\nCOMPREHENSIVE columns ({len(COMPREHENSIVE_COLUMNS)} total):\")\n",
    "    print(\", \".join(COMPREHENSIVE_COLUMNS))\n",
    "    \n",
    "    print(f\"\\nAdditional columns in COMPREHENSIVE (not in RESTRICTED):\")\n",
    "    additional_cols = [col for col in COMPREHENSIVE_COLUMNS if col not in RESTRICTED_COLUMNS]\n",
    "    print(\", \".join(additional_cols))\n",
    "    \n",
    "    # Show files to be processed\n",
    "    campaign_files = glob.glob(os.path.join(RAW_PATH, \"*.csv\"))\n",
    "    print(f\"\\nFiles to process ({len(campaign_files)} total):\")\n",
    "    for file_path in campaign_files:\n",
    "        filename = os.path.basename(file_path)\n",
    "        campaign_name = extract_campaign_name(filename)\n",
    "        print(f\"  {filename} → {campaign_name}_restricted.csv, {campaign_name}_comprehensive.csv\")\n",
    "\n",
    "# Run the processing\n",
    "if __name__ == \"__main__\":\n",
    "    # Display summary first\n",
    "    display_summary()\n",
    "    \n",
    "    # Process all files\n",
    "    process_campaign_files()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CAMPAIGN FILTERING COMPLETE!\")\n",
    "    print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
