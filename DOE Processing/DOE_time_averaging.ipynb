{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830f6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import icartt\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513f3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_average(df, COL_REF, window_size=None, time_step=None):\n",
    "   \"\"\"\n",
    "   Complete time averaging function that averages ALL data points in the dataset,\n",
    "   not just those around existing COL_REF values.\n",
    "   \n",
    "   Parameters:\n",
    "   df: pandas DataFrame with columns Date, UTC, COL_REF, and other data columns\n",
    "   COL_REF: string, name of the reference column \n",
    "   window_size: int or None, half-window size for averaging around each time point.\n",
    "                If None, automatically detected from COL_REF spacing or data spacing.\n",
    "   time_step: int or None, time step between averaging points.\n",
    "              If None, automatically detected from COL_REF spacing or data spacing.\n",
    "   \n",
    "   Returns:\n",
    "   pandas DataFrame with averaged values at regular time intervals\n",
    "   \"\"\"\n",
    "   import pandas as pd\n",
    "   import numpy as np\n",
    "   import time\n",
    "   \n",
    "   print(\"Starting complete time averaging...\")\n",
    "   start_time = time.time()\n",
    "   \n",
    "   # Make a copy to avoid modifying original\n",
    "   df_work = df.copy()\n",
    "   \n",
    "   # Store original column order\n",
    "   original_column_order = list(df.columns)\n",
    "   \n",
    "   # Step 1: Determine window_size and time_step from existing COL_REF data\n",
    "   ref_mask = df_work[COL_REF].notna() & (df_work[COL_REF] != '')\n",
    "   ref_points = df_work[ref_mask].copy()\n",
    "   \n",
    "   if len(ref_points) >= 2:\n",
    "       ref_points_sorted = ref_points.sort_values(['Date', 'UTC'])\n",
    "       first_utc = ref_points_sorted.iloc[0]['UTC']\n",
    "       second_utc = ref_points_sorted.iloc[1]['UTC']\n",
    "       detected_spacing = abs(second_utc - first_utc)\n",
    "       \n",
    "       if window_size is None:\n",
    "           window_size = max(1, detected_spacing // 2)\n",
    "       if time_step is None:\n",
    "           time_step = detected_spacing\n",
    "           \n",
    "       print(f\"Detected from COL_REF - Spacing: {detected_spacing}, Window: {window_size}, Time step: {time_step}\")\n",
    "   else:\n",
    "       # Fall back to detecting from overall data spacing\n",
    "       print(\"Insufficient COL_REF data, detecting from overall data...\")\n",
    "       df_sorted_sample = df_work.sort_values(['Date', 'UTC']).head(1000)  # Sample for speed\n",
    "       utc_diffs = df_sorted_sample['UTC'].diff().dropna()\n",
    "       median_spacing = utc_diffs.median()\n",
    "       \n",
    "       if window_size is None:\n",
    "           window_size = max(1, median_spacing // 2)\n",
    "       if time_step is None:\n",
    "           time_step = median_spacing\n",
    "           \n",
    "       print(f\"Detected from data - Median spacing: {median_spacing}, Window: {window_size}, Time step: {time_step}\")\n",
    "   \n",
    "   # Step 2: Sort data once by Date and UTC for efficient processing\n",
    "   print(\"Sorting data...\")\n",
    "   df_sorted = df_work.sort_values(['Date', 'UTC']).reset_index(drop=True)\n",
    "   sort_time = time.time()\n",
    "   print(f\"Sorting completed in {sort_time - start_time:.1f}s\")\n",
    "   \n",
    "   # Get column information\n",
    "   key_columns = ['Date', 'UTC', COL_REF]\n",
    "   numeric_columns = []\n",
    "   non_numeric_columns = []\n",
    "\n",
    "   for col in df_sorted.columns:\n",
    "       if col not in key_columns:\n",
    "           if col in ['Organization', 'Campaign']:\n",
    "               non_numeric_columns.append(col)\n",
    "           else:\n",
    "               numeric_columns.append(col)\n",
    "   \n",
    "   # Convert numeric data columns to numeric\n",
    "   print(\"Converting columns to numeric...\")\n",
    "   numeric_data = {}\n",
    "   for col in numeric_columns:\n",
    "       numeric_data[col] = pd.to_numeric(df_sorted[col], errors='coerce')\n",
    "   \n",
    "   # Step 3: Create complete time grid for each date\n",
    "   unique_dates = df_sorted['Date'].unique()\n",
    "   all_results = []\n",
    "   \n",
    "   print(f\"Processing {len(unique_dates)} unique dates...\")\n",
    "   \n",
    "   for date_idx, date in enumerate(unique_dates):\n",
    "       if date_idx % max(1, len(unique_dates) // 10) == 0:\n",
    "           elapsed = time.time() - start_time\n",
    "           print(f\"Processing date {date_idx + 1}/{len(unique_dates)} ({(date_idx/len(unique_dates)*100):.1f}%) - Elapsed: {elapsed:.1f}s\")\n",
    "       \n",
    "       # Get all data for this date\n",
    "       date_mask = df_sorted['Date'] == date\n",
    "       date_data = df_sorted[date_mask].copy()\n",
    "       \n",
    "       if len(date_data) == 0:\n",
    "           continue\n",
    "       \n",
    "       # Create time grid aligned to COL_REF for this date\n",
    "       min_utc = date_data['UTC'].min()\n",
    "       max_utc = date_data['UTC'].max()\n",
    "       \n",
    "       # Find COL_REF points for this date to determine alignment\n",
    "       date_ref_points = ref_points[ref_points['Date'] == date] if len(ref_points) > 0 else pd.DataFrame()\n",
    "       \n",
    "       if len(date_ref_points) > 0:\n",
    "           # Align grid to COL_REF spacing but extend to cover full range\n",
    "           first_ref_utc = date_ref_points['UTC'].min()\n",
    "           \n",
    "           # Calculate how many steps to go back to cover min_utc\n",
    "           steps_back = int(np.ceil((first_ref_utc - min_utc) / time_step))\n",
    "           grid_start = first_ref_utc - (steps_back * time_step)\n",
    "           \n",
    "           # Create grid covering full range with COL_REF alignment\n",
    "           time_points = np.arange(grid_start, max_utc + time_step, time_step)\n",
    "       else:\n",
    "           # No COL_REF for this date, use regular grid\n",
    "           time_points = np.arange(min_utc, max_utc + time_step, time_step)\n",
    "       \n",
    "       date_data_indices = np.where(date_mask)[0]\n",
    "       date_utc_values = date_data['UTC'].values\n",
    "       \n",
    "       # Step 4: For each time point, create averaged data\n",
    "       for time_point in time_points:\n",
    "           window_start = time_point - window_size\n",
    "           window_end = time_point + window_size\n",
    "           \n",
    "           # Use searchsorted for fast boundary finding\n",
    "           start_idx = np.searchsorted(date_utc_values, window_start, side='left')\n",
    "           end_idx = np.searchsorted(date_utc_values, window_end, side='right')\n",
    "           \n",
    "           if start_idx >= end_idx:\n",
    "               continue\n",
    "               \n",
    "           # Get the actual DataFrame indices for this window\n",
    "           window_indices = date_data_indices[start_idx:end_idx]\n",
    "           \n",
    "           # Step 5: Create averaged row\n",
    "           averaged_row = {\n",
    "               'Date': date,\n",
    "               'UTC': time_point\n",
    "           }\n",
    "           \n",
    "           # Handle COL_REF specially - only include if there's actual data\n",
    "           col_ref_values = df_sorted.loc[window_indices, COL_REF]\n",
    "           col_ref_valid = col_ref_values.dropna()\n",
    "           col_ref_valid = col_ref_valid[col_ref_valid != '']\n",
    "           \n",
    "           if len(col_ref_valid) > 0:\n",
    "               # If COL_REF data exists in window, average it\n",
    "               averaged_row[COL_REF] = pd.to_numeric(col_ref_valid, errors='coerce').mean()\n",
    "           else:\n",
    "               # If no COL_REF data, leave as NaN\n",
    "               averaged_row[COL_REF] = np.nan\n",
    "           \n",
    "           # Average numeric data columns\n",
    "           for col in numeric_columns:\n",
    "               col_values = numeric_data[col].iloc[window_indices]\n",
    "               valid_values = col_values.dropna()\n",
    "               if len(valid_values) > 0:\n",
    "                   averaged_row[col] = valid_values.mean()\n",
    "               else:\n",
    "                   averaged_row[col] = np.nan\n",
    "           \n",
    "           # Handle non-numeric columns (Organization, Campaign)\n",
    "           for col in non_numeric_columns:\n",
    "               col_values = df_sorted.loc[window_indices, col]\n",
    "               non_null_values = col_values.dropna()\n",
    "               if len(non_null_values) > 0:\n",
    "                   averaged_row[col] = non_null_values.iloc[0]\n",
    "               else:\n",
    "                   averaged_row[col] = np.nan\n",
    "           \n",
    "           all_results.append(averaged_row)\n",
    "   \n",
    "   # Step 6: Create result DataFrame\n",
    "   print(\"Creating result DataFrame...\")\n",
    "   result_df = pd.DataFrame(all_results)\n",
    "   \n",
    "   # Preserve original column order\n",
    "   if len(result_df) > 0:\n",
    "       available_cols = [col for col in original_column_order if col in result_df.columns]\n",
    "       result_df = result_df[available_cols]\n",
    "   \n",
    "   total_time = time.time() - start_time\n",
    "   print(f\"\\nComplete processing finished!\")\n",
    "   print(f\"Total time: {total_time:.1f}s ({total_time/60:.1f} minutes)\")\n",
    "   print(f\"Generated {len(all_results)} averaged time points\")\n",
    "   print(f\"Points with COL_REF data: {result_df[COL_REF].notna().sum()}\")\n",
    "   print(f\"Points without COL_REF data: {result_df[COL_REF].isna().sum()}\")\n",
    "   \n",
    "   return result_df\n",
    "\n",
    "def read_time_average_list(csv_path):\n",
    "   \"\"\"\n",
    "   Reads the time_average_list.csv and returns a dictionary mapping campaign names to column names.\n",
    "   \n",
    "   Parameters:\n",
    "   csv_path (str): Path to the time_average_list.csv file\n",
    "   \n",
    "   Returns:\n",
    "   dict: Dictionary with campaign names as keys and column names as values\n",
    "   \"\"\"\n",
    "   import pandas as pd\n",
    "   \n",
    "   # Read the CSV file\n",
    "   df = pd.read_csv(csv_path)\n",
    "   \n",
    "   # Create dictionary mapping from 'campaign' to 'column_name'\n",
    "   time_average_dict = dict(zip(df['campaign'], df['column_name']))\n",
    "   \n",
    "   return time_average_dict\n",
    "\n",
    "# Usage example:\n",
    "# time_average_mapping = read_time_average_list('time_average_list.csv')\n",
    "# print(time_average_mapping)\n",
    "# \n",
    "# # To get column name for a specific campaign:\n",
    "# col_ref = time_average_mapping.get('ACMEV', 'BC_Mass')  # Default to 'BC_Mass' if not found\n",
    "\n",
    "def standardize_campaign_dataframes(avg_path):\n",
    "    \"\"\"\n",
    "    Read all campaign_avg.csv files and standardize their column structure.\n",
    "    \n",
    "    Parameters:\n",
    "    avg_path: string, path to directory containing campaign_avg.csv files\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary with campaign names as keys and standardized DataFrames as values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the required columns in order\n",
    "    required_columns_base = [\n",
    "        'Organization',\n",
    "        'Campaign', \n",
    "        'UTC',\n",
    "        'Date',\n",
    "        'Latitude',\n",
    "        'Longitude',\n",
    "        'Altitude',\n",
    "        'Temperature',\n",
    "        'Rel_humidity',\n",
    "        'Pressure',\n",
    "        'U',\n",
    "        'V', \n",
    "        'W',\n",
    "        'Supersaturation',\n",
    "        'Number_Concentration',\n",
    "        'CNgt3nm',\n",
    "        'CNgt10nm',\n",
    "        'BC_Mass',\n",
    "        'LWC'\n",
    "    ]\n",
    "    \n",
    "    # Scattering and absorption columns\n",
    "    scattering_absorption_columns = [\n",
    "        'Sc450_total',\n",
    "        'Sc550_total', \n",
    "        'Sc700_total',\n",
    "        'Abs470_total',\n",
    "        'Abs532_total',\n",
    "        'Abs660_total'\n",
    "    ]\n",
    "    \n",
    "    # Find all campaign_avg.csv files\n",
    "    pattern = os.path.join(avg_path, \"*_avg.csv\")\n",
    "    avg_files = glob.glob(pattern)\n",
    "    \n",
    "    if not avg_files:\n",
    "        print(f\"No *_avg.csv files found in {avg_path}\")\n",
    "        return {}\n",
    "    \n",
    "    print(f\"Found {len(avg_files)} campaign average files\")\n",
    "    \n",
    "    standardized_dfs = {}\n",
    "    \n",
    "    for file_path in avg_files:\n",
    "        # Extract campaign name from filename\n",
    "        filename = os.path.basename(file_path)\n",
    "        campaign_name = filename.replace('_avg.csv', '')\n",
    "        \n",
    "        print(f\"\\nProcessing: {campaign_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  Original shape: {df.shape}\")\n",
    "            print(f\"  Original columns: {len(df.columns)}\")\n",
    "            \n",
    "            # Get existing bin and cbin columns\n",
    "            existing_bin_cols = [col for col in df.columns if col.startswith('bin') and col != 'bin']\n",
    "            existing_cbin_cols = [col for col in df.columns if col.startswith('cbin')]\n",
    "            \n",
    "            print(f\"  Found {len(existing_bin_cols)} bin columns, {len(existing_cbin_cols)} cbin columns\")\n",
    "            \n",
    "            # Determine bin and cbin columns to use\n",
    "            # For bin columns (minimum 15)\n",
    "            if existing_bin_cols:\n",
    "                # Sort existing bin columns naturally (bin1, bin2, ..., bin10, bin11, etc.)\n",
    "                def natural_sort_key(col):\n",
    "                    import re\n",
    "                    numbers = re.findall(r'\\d+', col)\n",
    "                    return int(numbers[0]) if numbers else 0\n",
    "                \n",
    "                existing_bin_cols.sort(key=natural_sort_key)\n",
    "                max_bin_num = max([int(''.join(filter(str.isdigit, col))) for col in existing_bin_cols if any(c.isdigit() for c in col)])\n",
    "                final_bin_count = max(15, max_bin_num)\n",
    "            else:\n",
    "                final_bin_count = 15\n",
    "            \n",
    "            # For cbin columns (minimum 20)  \n",
    "            if existing_cbin_cols:\n",
    "                existing_cbin_cols.sort(key=natural_sort_key)\n",
    "                max_cbin_num = max([int(''.join(filter(str.isdigit, col))) for col in existing_cbin_cols if any(c.isdigit() for c in col)])\n",
    "                final_cbin_count = max(20, max_cbin_num)\n",
    "            else:\n",
    "                final_cbin_count = 20\n",
    "            \n",
    "            # Generate bin and cbin column lists\n",
    "            bin_columns = [f'bin{i}' for i in range(1, final_bin_count + 1)]\n",
    "            cbin_columns = [f'cbin{i}' for i in range(1, final_cbin_count + 1)]\n",
    "            \n",
    "            print(f\"  Will use {len(bin_columns)} bin columns (bin1-bin{final_bin_count})\")\n",
    "            print(f\"  Will use {len(cbin_columns)} cbin columns (cbin1-cbin{final_cbin_count})\")\n",
    "            \n",
    "            # Create the complete required column list\n",
    "            required_columns = (required_columns_base + \n",
    "                              cbin_columns + \n",
    "                              bin_columns + \n",
    "                              scattering_absorption_columns)\n",
    "            \n",
    "            # Create a new DataFrame with all required columns\n",
    "            standardized_df = pd.DataFrame(columns=required_columns)\n",
    "            \n",
    "            # Copy existing data for columns that exist\n",
    "            for col in required_columns:\n",
    "                if col in df.columns:\n",
    "                    standardized_df[col] = df[col]\n",
    "                else:\n",
    "                    # Add missing column with NaN values\n",
    "                    standardized_df[col] = np.nan\n",
    "            \n",
    "            # Ensure the DataFrame has the same number of rows as original\n",
    "            if len(df) > 0:\n",
    "                standardized_df = standardized_df.reindex(range(len(df)))\n",
    "                \n",
    "                # Re-copy the data to ensure proper length\n",
    "                for col in required_columns:\n",
    "                    if col in df.columns:\n",
    "                        standardized_df[col] = df[col].values\n",
    "            \n",
    "            print(f\"  Standardized shape: {standardized_df.shape}\")\n",
    "            print(f\"  Added {len(required_columns) - len(df.columns)} missing columns\")\n",
    "            \n",
    "            # Store the standardized DataFrame\n",
    "            standardized_dfs[campaign_name] = standardized_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ERROR processing {campaign_name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nSuccessfully standardized {len(standardized_dfs)} campaign datasets\")\n",
    "    return standardized_dfs\n",
    "\n",
    "def save_standardized_dataframes(standardized_dfs, avg_path):\n",
    "    \"\"\"\n",
    "    Save the standardized DataFrames back to the original CSV files, replacing them.\n",
    "    \n",
    "    Parameters:\n",
    "    standardized_dfs: dict, Dictionary of standardized DataFrames\n",
    "    avg_path: string, Directory containing the original campaign_avg.csv files\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nReplacing original files in: {avg_path}\")\n",
    "    \n",
    "    for campaign_name, df in standardized_dfs.items():\n",
    "        output_file = os.path.join(avg_path, f\"{campaign_name}_avg.csv\")\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"  Replaced: {campaign_name}_avg.csv ({df.shape[0]} rows, {df.shape[1]} columns)\")\n",
    "    \n",
    "    print(f\"All {len(standardized_dfs)} files have been replaced with standardized versions!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6746aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded time averaging mapping for 1 campaigns\n",
      "\n",
      "==================================================\n",
      "Processing campaign: BBOP\n",
      "Using reference column: BC_Mass\n",
      "==================================================\n",
      "Reading: C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\renamed_data\\BBOP_renamed.csv\n",
      "Loaded 454849 rows with 145 columns\n",
      "Starting time averaging with reference column: BC_Mass\n",
      "Starting complete time averaging...\n",
      "Detected from COL_REF - Spacing: 10.0, Window: 5.0, Time step: 10.0\n",
      "Sorting data...\n",
      "Sorting completed in 0.4s\n",
      "Converting columns to numeric...\n",
      "Processing 35 unique dates...\n",
      "Processing date 1/35 (0.0%) - Elapsed: 0.4s\n",
      "Processing date 4/35 (8.6%) - Elapsed: 79.5s\n",
      "Processing date 7/35 (17.1%) - Elapsed: 182.4s\n",
      "Processing date 10/35 (25.7%) - Elapsed: 241.9s\n",
      "Processing date 13/35 (34.3%) - Elapsed: 370.1s\n",
      "Processing date 16/35 (42.9%) - Elapsed: 490.4s\n",
      "Processing date 19/35 (51.4%) - Elapsed: 587.3s\n",
      "Processing date 22/35 (60.0%) - Elapsed: 640.8s\n",
      "Processing date 25/35 (68.6%) - Elapsed: 716.6s\n",
      "Processing date 28/35 (77.1%) - Elapsed: 818.7s\n",
      "Processing date 31/35 (85.7%) - Elapsed: 913.4s\n",
      "Processing date 34/35 (94.3%) - Elapsed: 1015.9s\n",
      "Creating result DataFrame...\n",
      "\n",
      "Complete processing finished!\n",
      "Total time: 1071.0s (17.8 minutes)\n",
      "Generated 64847 averaged time points\n",
      "Points with COL_REF data: 33906\n",
      "Points without COL_REF data: 30941\n",
      "Saving averaged data to: C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\time_averaged_data\\BBOP_avg.csv\n",
      "Successfully saved 64847 averaged time points\n",
      "\n",
      "==================================================\n",
      "Time averaging processing completed!\n",
      "Check output directory: C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\time_averaged_data\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "raw_path = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\renamed_data\"\n",
    "avg_path = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\time_averaged_data\"\n",
    "time_average_list_path = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\time_average_list\\time_average_list.csv\"\n",
    "\n",
    "# Read the time averaging mapping\n",
    "time_average_mapping = read_time_average_list(time_average_list_path)\n",
    "print(f\"Loaded time averaging mapping for {len(time_average_mapping)} campaigns\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(avg_path, exist_ok=True)\n",
    "\n",
    "# Process each campaign in the mapping\n",
    "for campaign_name, col_ref in time_average_mapping.items():\n",
    "   print(f\"\\n{'='*50}\")\n",
    "   print(f\"Processing campaign: {campaign_name}\")\n",
    "   print(f\"Using reference column: {col_ref}\")\n",
    "   print(f\"{'='*50}\")\n",
    "   \n",
    "   # Define file paths\n",
    "   input_path = os.path.join(raw_path, f\"{campaign_name}_renamed.csv\")\n",
    "   output_path = os.path.join(avg_path, f\"{campaign_name}_avg.csv\")\n",
    "   \n",
    "   # Check if input file exists\n",
    "   if not os.path.exists(input_path):\n",
    "       print(f\"WARNING: Input file not found: {input_path}\")\n",
    "       print(\"Skipping this campaign...\")\n",
    "       continue\n",
    "   \n",
    "   try:\n",
    "       # Read the renamed campaign data\n",
    "       print(f\"Reading: {input_path}\")\n",
    "       df = pd.read_csv(input_path)\n",
    "       print(f\"Loaded {len(df)} rows with {len(df.columns)} columns\")\n",
    "       \n",
    "       # Check if the reference column exists\n",
    "       if col_ref not in df.columns:\n",
    "           print(f\"WARNING: Reference column '{col_ref}' not found in {campaign_name}\")\n",
    "           print(f\"Available columns: {list(df.columns)}\")\n",
    "           print(\"Skipping this campaign...\")\n",
    "           continue\n",
    "       \n",
    "       # Perform time averaging\n",
    "       print(f\"Starting time averaging with reference column: {col_ref}\")\n",
    "       df_avg = time_average(df, col_ref)\n",
    "       \n",
    "       # Save the averaged data\n",
    "       print(f\"Saving averaged data to: {output_path}\")\n",
    "       df_avg.to_csv(output_path, index=False)\n",
    "       print(f\"Successfully saved {len(df_avg)} averaged time points\")\n",
    "       \n",
    "   except Exception as e:\n",
    "       print(f\"ERROR processing {campaign_name}: {str(e)}\")\n",
    "       print(\"Skipping this campaign...\")\n",
    "       continue\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Time averaging processing completed!\")\n",
    "print(f\"Check output directory: {avg_path}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e508110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting standardization process...\n",
      "Found 9 campaign average files\n",
      "\n",
      "Processing: ACE-ENA\n",
      "  Original shape: (546787, 76)\n",
      "  Original columns: 76\n",
      "  Found 30 bin columns, 21 cbin columns\n",
      "  Will use 30 bin columns (bin1-bin30)\n",
      "  Will use 21 cbin columns (cbin1-cbin21)\n",
      "  Standardized shape: (546787, 76)\n",
      "  Added 0 missing columns\n",
      "\n",
      "Processing: ACMEV\n",
      "  Original shape: (55849, 142)\n",
      "  Original columns: 142\n",
      "  Found 87 bin columns, 30 cbin columns\n",
      "  Will use 87 bin columns (bin1-bin87)\n",
      "  Will use 30 cbin columns (cbin1-cbin30)\n",
      "  Standardized shape: (55849, 142)\n",
      "  Added 0 missing columns\n",
      "\n",
      "Processing: BBOP\n",
      "  Original shape: (64847, 145)\n",
      "  Original columns: 145\n",
      "  Found 91 bin columns, 30 cbin columns\n",
      "  Will use 91 bin columns (bin1-bin91)\n",
      "  Will use 30 cbin columns (cbin1-cbin30)\n",
      "  Standardized shape: (64847, 146)\n",
      "  Added 1 missing columns\n",
      "\n",
      "Processing: CACTI\n",
      "  Original shape: (38596, 145)\n",
      "  Original columns: 145\n",
      "  Found 99 bin columns, 21 cbin columns\n",
      "  Will use 99 bin columns (bin1-bin99)\n",
      "  Will use 21 cbin columns (cbin1-cbin21)\n",
      "  Standardized shape: (38596, 145)\n",
      "  Added 0 missing columns\n",
      "\n",
      "Processing: CARES\n",
      "  Original shape: (2838, 141)\n",
      "  Original columns: 141\n",
      "  Found 96 bin columns, 20 cbin columns\n",
      "  Will use 96 bin columns (bin1-bin96)\n",
      "  Will use 20 cbin columns (cbin1-cbin20)\n",
      "  Standardized shape: (2838, 141)\n",
      "  Added 0 missing columns\n",
      "\n",
      "Processing: GOAMAZON\n",
      "  Original shape: (331893, 154)\n",
      "  Original columns: 154\n",
      "  Found 99 bin columns, 30 cbin columns\n",
      "  Will use 99 bin columns (bin1-bin99)\n",
      "  Will use 30 cbin columns (cbin1-cbin30)\n",
      "  Standardized shape: (331893, 154)\n",
      "  Added 0 missing columns\n",
      "\n",
      "Processing: ISDAC\n",
      "  Original shape: (441569, 154)\n",
      "  Original columns: 154\n",
      "  Found 99 bin columns, 30 cbin columns\n",
      "  Will use 99 bin columns (bin1-bin99)\n",
      "  Will use 30 cbin columns (cbin1-cbin30)\n",
      "  Standardized shape: (441569, 154)\n",
      "  Added 0 missing columns\n",
      "\n",
      "Processing: TCAP2012\n",
      "  Original shape: (15523, 70)\n",
      "  Original columns: 70\n",
      "  Found 15 bin columns, 30 cbin columns\n",
      "  Will use 15 bin columns (bin1-bin15)\n",
      "  Will use 30 cbin columns (cbin1-cbin30)\n",
      "  Standardized shape: (15523, 70)\n",
      "  Added 0 missing columns\n",
      "\n",
      "Processing: TCAP2013\n",
      "  Original shape: (18901, 154)\n",
      "  Original columns: 154\n",
      "  Found 99 bin columns, 30 cbin columns\n",
      "  Will use 99 bin columns (bin1-bin99)\n",
      "  Will use 30 cbin columns (cbin1-cbin30)\n",
      "  Standardized shape: (18901, 154)\n",
      "  Added 0 missing columns\n",
      "\n",
      "Successfully standardized 9 campaign datasets\n",
      "\n",
      "Standardization Summary:\n",
      "--------------------------------------------------\n",
      "ACE-ENA: 546787 rows, 76 columns (35597452 non-empty values)\n",
      "ACMEV: 55849 rows, 142 columns (3419078 non-empty values)\n",
      "BBOP: 64847 rows, 146 columns (2585279 non-empty values)\n",
      "CACTI: 38596 rows, 145 columns (3822954 non-empty values)\n",
      "CARES: 2838 rows, 141 columns (204296 non-empty values)\n",
      "GOAMAZON: 331893 rows, 154 columns (47226325 non-empty values)\n",
      "ISDAC: 441569 rows, 154 columns (58469940 non-empty values)\n",
      "TCAP2012: 15523 rows, 70 columns (653330 non-empty values)\n",
      "TCAP2013: 18901 rows, 154 columns (2418400 non-empty values)\n",
      "\n",
      "Replacing original files in: C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\time_averaged_data\n",
      "  Replaced: ACE-ENA_avg.csv (546787 rows, 76 columns)\n",
      "  Replaced: ACMEV_avg.csv (55849 rows, 142 columns)\n",
      "  Replaced: BBOP_avg.csv (64847 rows, 146 columns)\n",
      "  Replaced: CACTI_avg.csv (38596 rows, 145 columns)\n",
      "  Replaced: CARES_avg.csv (2838 rows, 141 columns)\n",
      "  Replaced: GOAMAZON_avg.csv (331893 rows, 154 columns)\n",
      "  Replaced: ISDAC_avg.csv (441569 rows, 154 columns)\n",
      "  Replaced: TCAP2012_avg.csv (15523 rows, 70 columns)\n",
      "  Replaced: TCAP2013_avg.csv (18901 rows, 154 columns)\n",
      "All 9 files have been replaced with standardized versions!\n"
     ]
    }
   ],
   "source": [
    "# Define path\n",
    "avg_path = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\time_averaged_data\"\n",
    "\n",
    "# Standardize all campaign DataFrames\n",
    "print(\"Starting standardization process...\")\n",
    "standardized_campaigns = standardize_campaign_dataframes(avg_path)\n",
    "\n",
    "# Display summary information\n",
    "if standardized_campaigns:\n",
    "    print(f\"\\nStandardization Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    for campaign_name, df in standardized_campaigns.items():\n",
    "        non_empty_cols = df.count().sum()\n",
    "        total_cols = len(df.columns)\n",
    "        print(f\"{campaign_name}: {df.shape[0]} rows, {total_cols} columns ({non_empty_cols} non-empty values)\")\n",
    "    \n",
    "    # Replace original files with standardized versions\n",
    "    save_standardized_dataframes(standardized_campaigns, avg_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
