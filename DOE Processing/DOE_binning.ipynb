{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830f6249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import icartt\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from netCDF4 import Dataset\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import glob\n",
    "from math import pi\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0644c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SECTION 2: Create index bins\n",
    "\n",
    "def create_index_bin(old_bin_diams, new_bin_diams):\n",
    "    \"\"\"\n",
    "    Create index bins for new bin diameters based on old bin diameters,\n",
    "    with each bin containing only indices exclusive to it.\n",
    "    \n",
    "    Parameters:\n",
    "    old_bin_diams (list): List of original bin diameters.\n",
    "    new_bin_diams (list): List of desired new bin diameters.\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of lists containing indices of old bins corresponding to each new bin diameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create list to store highest index for each new bin diameter\n",
    "    max_indices = []\n",
    "    \n",
    "    for diameter in new_bin_diams:\n",
    "        # Find the highest index where old_bin_diams is less than the current diameter\n",
    "        indices = [i for i, old_diam in enumerate(old_bin_diams) if old_diam < diameter]\n",
    "        max_index = max(indices) if indices else -1\n",
    "        max_indices.append(max_index)\n",
    "    \n",
    "    # Create exclusive bins based on max indices\n",
    "    index_bins = []\n",
    "    prev_max = -1\n",
    "    \n",
    "    for current_max in max_indices:\n",
    "        # Only include indices that fall between prev_max and current_max\n",
    "        exclusive_indices = list(range(prev_max + 1, current_max + 1))\n",
    "        index_bins.append(exclusive_indices)\n",
    "        prev_max = current_max\n",
    "    \n",
    "    # Add remaining indices that weren't captured\n",
    "    remaining_indices = list(range(prev_max + 1, len(old_bin_diams)))\n",
    "    index_bins.append(remaining_indices)\n",
    "    \n",
    "    return index_bins\n",
    "\n",
    "def bin_name_list(num_bins, bin_type=\"Aerosol\"):\n",
    "    \"\"\"\n",
    "    Generate a list of bin names in the format \"bin1\", \"bin2\", ..., \"binN\"\n",
    "    \n",
    "    Args:\n",
    "        num_bins (int): Number of bins to generate names for\n",
    "        \n",
    "    Returns:\n",
    "        list: List of bin names as strings\n",
    "    \"\"\"\n",
    "    #if aerosol, print bin1, bin2, etc.\n",
    "    #if cloud, print cbin1, cbin2, etc.\n",
    "    if(bin_type == \"Aerosol\"):\n",
    "        return [f\"bin{i+1}\" for i in range(num_bins)]\n",
    "    else:\n",
    "        return [f\"cbin{i+1}\" for i in range(num_bins)]\n",
    "    \n",
    "\n",
    "#SECTION 3: Consolidate the bins to reduced # of bins\n",
    "def consolidate_bins(df, index_bins, bin_names, new_bin_count, bin_type=\"Aerosol\"):\n",
    "    \"\"\"\n",
    "    Consolidate original bins into new bins, replacing the original bin columns\n",
    "    at their original positions. Preserves empty values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the original bin data\n",
    "    index_bins (list): Lists of indices mapping old bins to new bins\n",
    "    bin_names (list): Original bin column names\n",
    "    new_bin_count (int): Number of new bins to create\n",
    "    bin_type (str): Type of bin for naming\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame with original bin columns replaced by new consolidated bin columns\n",
    "    \"\"\"\n",
    "    # Create a copy of the DataFrame to avoid modifying the original\n",
    "    result_df = df.copy()\n",
    "    \n",
    "    # Find position of first bin column to maintain ordering\n",
    "    all_columns = list(df.columns)\n",
    "    if bin_names[0] in all_columns:\n",
    "        first_bin_position = all_columns.index(bin_names[0])\n",
    "    else:\n",
    "        first_bin_position = 0  # Default to beginning if not found\n",
    "    \n",
    "    # Calculate the new bin values before modifying the DataFrame\n",
    "    new_bin_values = {}\n",
    "    new_bin_names = bin_name_list(new_bin_count, bin_type)\n",
    "    \n",
    "    for new_idx, indices in enumerate(index_bins):\n",
    "        # Filter out indices that are out of range for bin_names\n",
    "        valid_indices = [i for i in indices if i < len(bin_names)]\n",
    "        \n",
    "        # Get the old bin names corresponding to these valid indices\n",
    "        old_bin_columns = [bin_names[i] for i in valid_indices]\n",
    "        \n",
    "        if old_bin_columns:  # Check if there are any columns to sum\n",
    "            # Create a mask identifying where ALL source columns are empty/NA\n",
    "            all_empty_mask = df[old_bin_columns].isna().all(axis=1)\n",
    "            \n",
    "            # Sum the values\n",
    "            summed_values = df[old_bin_columns].sum(axis=1)\n",
    "            \n",
    "            # Where all original values were empty, set the result to NA\n",
    "            summed_values = summed_values.mask(all_empty_mask)\n",
    "            \n",
    "            new_bin_values[new_bin_names[new_idx]] = summed_values\n",
    "        else:\n",
    "            # If no old bins correspond to this new bin, fill with NA\n",
    "            new_bin_values[new_bin_names[new_idx]] = pd.Series(pd.NA, index=df.index)\n",
    "    \n",
    "    # Drop the original bin columns\n",
    "    result_df = result_df.drop(columns=bin_names)\n",
    "    \n",
    "    # Insert new bin columns at the original positions\n",
    "    for i, new_bin_name in enumerate(new_bin_names):\n",
    "        insert_position = first_bin_position + i\n",
    "        # Make sure we don't try to insert beyond the DataFrame's length\n",
    "        if insert_position <= len(result_df.columns):\n",
    "            result_df.insert(insert_position, new_bin_name, new_bin_values[new_bin_name])\n",
    "        else:\n",
    "            # If we run out of space, append to the end\n",
    "            result_df[new_bin_name] = new_bin_values[new_bin_name]\n",
    "\n",
    "# NEW: Fill NaN values with 0 when other bins in the same row have data\n",
    "    # Get all the new bin column names\n",
    "    all_new_bin_cols = [col for col in result_df.columns if \n",
    "                       (bin_type == \"Aerosol\" and col.startswith('bin') and col[3:].isdigit()) or\n",
    "                       (bin_type == \"Cloud\" and col.startswith('cbin') and col[4:].isdigit())]\n",
    "    \n",
    "    if all_new_bin_cols:\n",
    "        # For each row, check if any bin has data (not NaN)\n",
    "        has_any_data_mask = result_df[all_new_bin_cols].notna().any(axis=1)\n",
    "        \n",
    "        # For rows that have any bin data, fill NaN values with 0\n",
    "        for col in all_new_bin_cols:\n",
    "            # Only fill NaN with 0 where the row has some bin data\n",
    "            result_df.loc[has_any_data_mask, col] = result_df.loc[has_any_data_mask, col].fillna(0)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd1b8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_AEROSOL_BINS = [150, 169.8,192.1,217.5,246.1,278.6,315.3,356.8,403.9,457.1,517.3,585.5,662.7,750]\n",
    "NEW_CLOUD_BINS = [3, 8.3, 13.5, 18.8, 24, 29.3, 34.5, 39.8, 45]\n",
    "OUTPUT_PATH = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\binned_data\"\n",
    "INPUT_PATH = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\unit_corrected_data\"\n",
    "BIN_INFO_PATH = rf\"C:\\Users\\haika\\Desktop\\May_Research\\may_datasets\\bin_info_list\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c759184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bin_info_files():\n",
    "    \"\"\"\n",
    "    Read all bin info CSV files and return a dictionary with campaign names as keys\n",
    "    and bin diameter information as values.\n",
    "    \"\"\"\n",
    "    bin_info_dict = {}\n",
    "    \n",
    "    # Get all CSV files in the bin info directory\n",
    "    bin_info_files = glob.glob(os.path.join(BIN_INFO_PATH, \"*.csv\"))\n",
    "    \n",
    "    for file_path in bin_info_files:\n",
    "        # Extract campaign name from filename\n",
    "        campaign_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Store the dataframe in the dictionary\n",
    "            bin_info_dict[campaign_name] = df\n",
    "            \n",
    "            print(f\"Loaded bin info for campaign: {campaign_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "    \n",
    "    return bin_info_dict\n",
    "\n",
    "def parse_bin_diameters(bin_diams_str):\n",
    "    \"\"\"\n",
    "    Parse bin diameters string into a list of floats.\n",
    "    Handle empty or invalid entries.\n",
    "    \"\"\"\n",
    "    if pd.isna(bin_diams_str) or bin_diams_str.strip() == '':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Try to evaluate as a Python list\n",
    "        return ast.literal_eval(bin_diams_str)\n",
    "    except:\n",
    "        try:\n",
    "            # Try comma-separated values\n",
    "            return [float(x.strip()) for x in bin_diams_str.split(',')]\n",
    "        except:\n",
    "            print(f\"Warning: Could not parse bin diameters: {bin_diams_str}\")\n",
    "            return []\n",
    "\n",
    "def get_existing_bin_columns(df, bin_type):\n",
    "    \"\"\"\n",
    "    Get existing bin column names from dataframe based on bin type.\n",
    "    \"\"\"\n",
    "    if bin_type == \"Aerosol\":\n",
    "        # Look for columns like bin1, bin2, etc.\n",
    "        bin_cols = [col for col in df.columns if col.startswith('bin') and col[3:].isdigit()]\n",
    "    else:  # Cloud\n",
    "        # Look for columns like cbin1, cbin2, etc.\n",
    "        bin_cols = [col for col in df.columns if col.startswith('cbin') and col[4:].isdigit()]\n",
    "    \n",
    "    # Sort numerically\n",
    "    if bin_type == \"Aerosol\":\n",
    "        bin_cols.sort(key=lambda x: int(x[3:]))\n",
    "    else:\n",
    "        bin_cols.sort(key=lambda x: int(x[4:]))\n",
    "    \n",
    "    return bin_cols\n",
    "\n",
    "def ensure_minimum_bins(df, bin_type):\n",
    "    \"\"\"\n",
    "    Ensure minimum number of bin columns exist, creating empty ones if needed.\n",
    "    \"\"\"\n",
    "    if bin_type == \"Aerosol\":\n",
    "        min_bins = 15\n",
    "        prefix = \"bin\"\n",
    "    else:  # Cloud\n",
    "        min_bins = 10\n",
    "        prefix = \"cbin\"\n",
    "    \n",
    "    existing_bins = get_existing_bin_columns(df, bin_type)\n",
    "    \n",
    "    # Add missing bins with NA values\n",
    "    for i in range(1, min_bins + 1):\n",
    "        col_name = f\"{prefix}{i}\"\n",
    "        if col_name not in df.columns:\n",
    "            df[col_name] = pd.NA\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_campaign_binning():\n",
    "    \"\"\"\n",
    "    Process all campaign data files and apply binning transformations.\n",
    "    \"\"\"\n",
    "    print(\"=\"*50)\n",
    "    print(\"PROCESSING CAMPAIGN BINNING:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "    \n",
    "    # Read bin info files\n",
    "    bin_info_dict = read_bin_info_files()\n",
    "    \n",
    "    # Process each campaign\n",
    "    for campaign_name, bin_info_df in bin_info_dict.items():\n",
    "        print(f\"\\nProcessing campaign: {campaign_name}\")\n",
    "        \n",
    "        # Look for campaign data file in INPUT_PATH\n",
    "        input_file = os.path.join(INPUT_PATH, f\"{campaign_name}.csv\")\n",
    "        \n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"  No data file found: {campaign_name}.csv\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Read the campaign data\n",
    "            df = pd.read_csv(input_file)\n",
    "            print(f\"  Loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "            \n",
    "            # Process each bin type (Aerosol and Cloud)\n",
    "            for _, row in bin_info_df.iterrows():\n",
    "                bin_type = row['Type']\n",
    "                bin_diams_str = row['bin_diams']\n",
    "                \n",
    "                print(f\"  Processing {bin_type} bins...\")\n",
    "                \n",
    "                # Parse bin diameters\n",
    "                old_bin_diams = parse_bin_diameters(bin_diams_str)\n",
    "                \n",
    "                if not old_bin_diams:\n",
    "                    print(f\"    No bin diameters found for {bin_type}, ensuring minimum bins...\")\n",
    "                    df = ensure_minimum_bins(df, bin_type)\n",
    "                    continue\n",
    "                \n",
    "                # Get existing bin columns\n",
    "                existing_bin_cols = get_existing_bin_columns(df, bin_type)\n",
    "                \n",
    "                if not existing_bin_cols:\n",
    "                    print(f\"    No existing {bin_type} bin columns found\")\n",
    "                    df = ensure_minimum_bins(df, bin_type)\n",
    "                    continue\n",
    "                \n",
    "                # Determine new bins and create index mapping\n",
    "                if bin_type == \"Aerosol\":\n",
    "                    new_bin_diams = NEW_AEROSOL_BINS\n",
    "                    new_bin_count = max(15, len(NEW_AEROSOL_BINS) + 1)  # +1 for overflow bin\n",
    "                else:  # Cloud\n",
    "                    new_bin_diams = NEW_CLOUD_BINS\n",
    "                    new_bin_count = max(10, len(NEW_CLOUD_BINS) + 1)  # +1 for overflow bin\n",
    "                \n",
    "                # Create index bins\n",
    "                index_bins = create_index_bin(old_bin_diams, new_bin_diams)\n",
    "                \n",
    "                print(f\"    Consolidating {len(existing_bin_cols)} old bins into {new_bin_count} new bins\")\n",
    "                \n",
    "                # Consolidate bins\n",
    "                df = consolidate_bins(df, index_bins, existing_bin_cols, new_bin_count, bin_type)\n",
    "                \n",
    "                # Ensure minimum bins after consolidation\n",
    "                df = ensure_minimum_bins(df, bin_type)\n",
    "            \n",
    "            # Save the binned data\n",
    "            output_filename = f\"{campaign_name}_binned.csv\"\n",
    "            output_path = os.path.join(OUTPUT_PATH, output_filename)\n",
    "            df.to_csv(output_path, index=False)\n",
    "            \n",
    "            print(f\"  Saved binned data to: {output_filename}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing {campaign_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664abdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "PROCESSING CAMPAIGN BINNING:\n",
      "==================================================\n",
      "Loaded bin info for campaign: ACE-ENA\n",
      "Loaded bin info for campaign: ACMEV\n",
      "Loaded bin info for campaign: BBOP\n",
      "Loaded bin info for campaign: CACTI\n",
      "Loaded bin info for campaign: CARES\n",
      "Loaded bin info for campaign: GOAMAZON\n",
      "Loaded bin info for campaign: ISDAC\n",
      "Loaded bin info for campaign: TCAP2012\n",
      "Loaded bin info for campaign: TCAP2013\n",
      "\n",
      "Processing campaign: ACE-ENA\n",
      "  Loaded data with 546787 rows and 76 columns\n",
      "  Processing Aerosol bins...\n",
      "    Consolidating 30 old bins into 15 new bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haika\\AppData\\Local\\Temp\\ipykernel_19496\\3314582990.py:137: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df.loc[has_any_data_mask, col] = result_df.loc[has_any_data_mask, col].fillna(0)\n",
      "C:\\Users\\haika\\AppData\\Local\\Temp\\ipykernel_19496\\3314582990.py:137: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df.loc[has_any_data_mask, col] = result_df.loc[has_any_data_mask, col].fillna(0)\n",
      "C:\\Users\\haika\\AppData\\Local\\Temp\\ipykernel_19496\\3314582990.py:137: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df.loc[has_any_data_mask, col] = result_df.loc[has_any_data_mask, col].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Cloud bins...\n",
      "    Consolidating 21 old bins into 10 new bins\n",
      "  Saved binned data to: ACE-ENA_binned.csv\n",
      "\n",
      "Processing campaign: ACMEV\n",
      "  Loaded data with 55849 rows and 142 columns\n",
      "  Processing Aerosol bins...\n",
      "    Consolidating 87 old bins into 15 new bins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haika\\AppData\\Local\\Temp\\ipykernel_19496\\3314582990.py:137: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df.loc[has_any_data_mask, col] = result_df.loc[has_any_data_mask, col].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Processing Cloud bins...\n",
      "    Consolidating 30 old bins into 10 new bins\n",
      "  Saved binned data to: ACMEV_binned.csv\n",
      "\n",
      "Processing campaign: BBOP\n",
      "  Loaded data with 64847 rows and 146 columns\n",
      "  Processing Aerosol bins...\n",
      "    Consolidating 91 old bins into 15 new bins\n",
      "  Processing Cloud bins...\n",
      "    Consolidating 30 old bins into 10 new bins\n",
      "  Saved binned data to: BBOP_binned.csv\n",
      "\n",
      "Processing campaign: CACTI\n",
      "  Loaded data with 38596 rows and 145 columns\n",
      "  Processing Aerosol bins...\n",
      "    Consolidating 99 old bins into 15 new bins\n",
      "  Processing Cloud bins...\n",
      "    Consolidating 21 old bins into 10 new bins\n",
      "  Saved binned data to: CACTI_binned.csv\n",
      "\n",
      "Processing campaign: CARES\n",
      "  Loaded data with 2838 rows and 141 columns\n",
      "  Processing Aerosol bins...\n",
      "    Consolidating 96 old bins into 15 new bins\n",
      "  Processing Cloud bins...\n",
      "    Consolidating 20 old bins into 10 new bins\n",
      "  Saved binned data to: CARES_binned.csv\n",
      "\n",
      "Processing campaign: GOAMAZON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haika\\AppData\\Local\\Temp\\ipykernel_19496\\3314582990.py:137: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df.loc[has_any_data_mask, col] = result_df.loc[has_any_data_mask, col].fillna(0)\n",
      "C:\\Users\\haika\\AppData\\Local\\Temp\\ipykernel_19496\\3314582990.py:137: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  result_df.loc[has_any_data_mask, col] = result_df.loc[has_any_data_mask, col].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded data with 331893 rows and 154 columns\n",
      "  Processing Aerosol bins...\n",
      "    Consolidating 99 old bins into 15 new bins\n",
      "  Processing Cloud bins...\n",
      "    Consolidating 30 old bins into 10 new bins\n",
      "  Saved binned data to: GOAMAZON_binned.csv\n",
      "\n",
      "Processing campaign: ISDAC\n",
      "  Loaded data with 441569 rows and 154 columns\n",
      "  Processing Aerosol bins...\n",
      "    Consolidating 99 old bins into 15 new bins\n",
      "  Processing Cloud bins...\n",
      "    Consolidating 30 old bins into 10 new bins\n",
      "  Saved binned data to: ISDAC_binned.csv\n",
      "\n",
      "Processing campaign: TCAP2012\n",
      "  Loaded data with 15523 rows and 70 columns\n",
      "  Processing Aerosol bins...\n",
      "    No bin diameters found for Aerosol, ensuring minimum bins...\n",
      "  Processing Cloud bins...\n",
      "    Consolidating 30 old bins into 10 new bins\n",
      "  Saved binned data to: TCAP2012_binned.csv\n",
      "\n",
      "Processing campaign: TCAP2013\n",
      "  Loaded data with 18901 rows and 154 columns\n",
      "  Processing Aerosol bins...\n",
      "    Consolidating 99 old bins into 15 new bins\n",
      "  Processing Cloud bins...\n",
      "    Consolidating 30 old bins into 10 new bins\n",
      "  Saved binned data to: TCAP2013_binned.csv\n",
      "\n",
      "==================================================\n",
      "BINNING PROCESS COMPLETE!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the binning process\n",
    "if __name__ == \"__main__\":\n",
    "    process_campaign_binning()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"BINNING PROCESS COMPLETE!\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
